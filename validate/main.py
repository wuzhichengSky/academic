import os
import json
import torch
import clip
from torchvision import transforms
import re
import numpy as np
from PIL import Image
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

# ============ é…ç½® ============
device = "cuda" if torch.cuda.is_available() else "cpu"

image_root = r"D:\datasets\SUES-200-512x512-V2"
train_json = "./train.json"
test_json  = "./test.json"

batch_size = 16
epochs = 20

# æ–¹æ¡ˆAï¼šæŠ•å½±å±‚å¯ç”¨ç¨å¤§ lrï¼Œä½† logit_scale å¿…é¡»æ›´å°ï¼ˆparam groups å•ç‹¬è®¾ï¼‰
proj_lr = 5e-5
logit_scale_lr = 1e-6

weight_decay = 0.01
num_workers = 4

NORM_EPS = 1e-6
LOGIT_SCALE_MAX = float(np.log(100.0))  # ~4.6052

# ================= æµ‹è¯•éšæœºæ€§ transformï¼ˆä¿æŒä½ åŸæœ¬çš„é€»è¾‘ï¼‰ =================
test_transform = transforms.Compose([
    transforms.Resize(224, interpolation=Image.BICUBIC),
    transforms.RandomResizedCrop(
        224,
        scale=(0.9, 1.0),
        ratio=(0.95, 1.05)
    ),
    transforms.ColorJitter(
        brightness=0.1,
        contrast=0.1,
        saturation=0.1,
        hue=0.05
    ),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.48145466, 0.4578275, 0.40821073],
        std=[0.26862954, 0.26130258, 0.27577711]
    )
])

def clean_text(s: str) -> str:
    if s is None or str(s).strip() == "":
        return "empty description"
    s = str(s)
    s = re.sub(r"(?i)\bWord\s*count\s*:\s*\d+\.?$", "", s.strip())
    s = re.sub(r"\s+", " ", s).strip()
    return s

def parse_scene_from_path(rel_path: str) -> str:
    rel_path = rel_path.replace("\\", "/")
    parts = rel_path.split("/")
    # e.g. drone_view_512/0160/300/48.jpg -> scene=0160
    if len(parts) >= 2:
        return parts[1]
    return ""

def l2norm(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    return x / (x.norm(dim=-1, keepdim=True) + eps)

def is_finite_tensor(x: torch.Tensor) -> bool:
    return torch.isfinite(x).all().item()

class CLIPPairedDataset(Dataset):
    """
    æ–°æ ‡æ³¨æ ¼å¼ï¼ˆlist of dictï¼‰ï¼š
    [
      {"drone": "...", "satellite": "...", "caption": "..."},
      ...
    ]
    """
    def __init__(self, json_path, image_root, image_transform):
        with open(json_path, "r", encoding="utf-8") as f:
            self.data = json.load(f)

        self.image_root = image_root
        self.image_transform = image_transform

        self.drone_paths = []
        self.sat_paths = []
        self.texts = []
        self.gt_scenes = []

        for item in self.data:
            drone_rel = item["drone"]
            sat_rel   = item["satellite"]
            cap       = clean_text(item.get("caption", ""))

            self.drone_paths.append(drone_rel)
            self.sat_paths.append(sat_rel)
            self.texts.append(cap)
            self.gt_scenes.append(parse_scene_from_path(drone_rel))

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        drone_rel = self.drone_paths[idx]
        sat_rel   = self.sat_paths[idx]
        text      = self.texts[idx]

        drone_full = os.path.join(self.image_root, drone_rel)
        sat_full   = os.path.join(self.image_root, sat_rel)

        drone_img = Image.open(drone_full).convert("RGB")
        sat_img   = Image.open(sat_full).convert("RGB")

        drone_img = self.image_transform(drone_img)
        sat_img   = self.image_transform(sat_img)

        return drone_img, sat_img, text


def configure_trainable_params_A(model):
    """
    æ–¹æ¡ˆ Aï¼šå†»ç»“æ‰€æœ‰ï¼Œåªè®­ç»ƒæŠ•å½±å±‚ + logit_scale
    """
    for p in model.parameters():
        p.requires_grad = False

    # logit_scale
    if hasattr(model, "logit_scale") and model.logit_scale is not None:
        model.logit_scale.requires_grad = True
    else:
        raise RuntimeError("model.logit_scale not found")

    # text projection
    if hasattr(model, "text_projection") and model.text_projection is not None:
        model.text_projection.requires_grad = True
    else:
        raise RuntimeError("model.text_projection not found")

    # visual projection (ViT usually has visual.proj)
    if hasattr(model, "visual") and hasattr(model.visual, "proj") and model.visual.proj is not None:
        model.visual.proj.requires_grad = True
    else:
        raise RuntimeError("model.visual.proj not found")

    trainable = [(n, p) for n, p in model.named_parameters() if p.requires_grad]
    print("âœ… Trainable parameters (scheme A):")
    for n, p in trainable:
        print(f"  - {n}: shape={tuple(p.shape)}")
    return {n: p for n, p in trainable}


# ============ Training ============
def train():
    # 1. åŠ è½½æ¨¡å‹
    model, preprocess = clip.load(
        "ViT-B/32",
        device=device,
        download_root="../../clip_weights",
        jit=False
    )
    
    # ã€å…³é”®ä¿®æ”¹ã€‘å°†æ¨¡å‹è½¬æ¢ä¸º float32ï¼Œé˜²æ­¢ fp16 è®­ç»ƒæ—¶çš„æ¢¯åº¦æº¢å‡ºå¯¼è‡´ NaN
    if device == "cuda":
        model = model.float()

    # 2. é…ç½®å¯è®­ç»ƒå‚æ•°
    trainable = configure_trainable_params_A(model)

    train_dataset = CLIPPairedDataset(train_json, image_root, preprocess)
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=(device == "cuda"),
        drop_last=True
    )

    # 3. ä¼˜åŒ–å™¨é…ç½®
    optimizer = torch.optim.AdamW([
        {"params": [trainable["logit_scale"]], "lr": logit_scale_lr, "weight_decay": 0.0},
        {"params": [trainable["text_projection"]], "lr": proj_lr, "weight_decay": weight_decay},
        {"params": [trainable["visual.proj"]], "lr": proj_lr, "weight_decay": weight_decay},
    ])

    loss_fn = torch.nn.CrossEntropyLoss()

    # åˆå§‹ clamp
    with torch.no_grad():
        model.logit_scale.clamp_(0, LOGIT_SCALE_MAX)

    print("âœ… Start Training (paired: text + drone + satellite) - Force Float32 Mode")

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        valid_steps = 0
        skipped_steps = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for step, (drone_imgs, sat_imgs, texts) in enumerate(pbar):
            # æ•°æ®æ¬è¿
            drone_imgs = drone_imgs.to(device, non_blocking=True)
            sat_imgs   = sat_imgs.to(device, non_blocking=True)
            texts = [t if isinstance(t, str) else str(t) for t in texts]
            tokens = clip.tokenize(texts, truncate=True).to(device, non_blocking=True)

            # ---- encode ----
            # æ³¨æ„ï¼šå› ä¸ºæ¨¡å‹å·²ç»æ˜¯ float32ï¼Œè¿™é‡Œè®¡ç®—å‡ºæ¥çš„ feats ä¹Ÿæ˜¯ float32ï¼Œæ•°å€¼æ›´ç¨³å®š
            drone_feats = model.encode_image(drone_imgs)
            sat_feats   = model.encode_image(sat_imgs)
            text_feats  = model.encode_text(tokens)

            # normalize
            drone_feats = l2norm(drone_feats, NORM_EPS)
            sat_feats   = l2norm(sat_feats, NORM_EPS)
            text_feats  = l2norm(text_feats, NORM_EPS)

            # å®‰å…¨æ£€æŸ¥ï¼ˆå¦‚æœè½¬äº† float32ï¼Œè¿™é‡ŒåŸºæœ¬ä¸ä¼šå†è§¦å‘äº†ï¼‰
            if (not is_finite_tensor(drone_feats)) or (not is_finite_tensor(sat_feats)) or (not is_finite_tensor(text_feats)):
                skipped_steps += 1
                optimizer.zero_grad(set_to_none=True)
                continue

            # logit_scale clamp
            with torch.no_grad():
                model.logit_scale.clamp_(0, LOGIT_SCALE_MAX)
            logit_scale = model.logit_scale.exp()

            # Logits Calculation
            logits_t_d = logit_scale * (text_feats @ drone_feats.t())
            logits_d_t = logits_t_d.t()
            logits_t_s = logit_scale * (text_feats @ sat_feats.t())
            logits_s_t = logits_t_s.t()

            labels = torch.arange(drone_imgs.size(0), device=device)

            loss_drone = (loss_fn(logits_t_d, labels) + loss_fn(logits_d_t, labels)) / 2
            loss_sat   = (loss_fn(logits_t_s, labels) + loss_fn(logits_s_t, labels)) / 2
            loss = (loss_drone + loss_sat) / 2

            # Check Loss NaN
            if not torch.isfinite(loss).item():
                skipped_steps += 1
                optimizer.zero_grad(set_to_none=True)
                # print("Warning: NaN loss detected, skipping step.") # è°ƒè¯•æ—¶å¯ä»¥æ‰“å¼€
                continue

            optimizer.zero_grad(set_to_none=True)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                [p for p in model.parameters() if p.requires_grad],
                max_norm=1.0
            )

            optimizer.step()

            # åå¤„ç† clamp
            with torch.no_grad():
                model.logit_scale.clamp_(0, LOGIT_SCALE_MAX)

            epoch_loss += loss.item()
            valid_steps += 1

            if epoch == 0 and step < 3:
                pbar.set_postfix({
                    "loss": f"{loss.item():.4f}",
                    "exp(scale)": f"{model.logit_scale.exp().item():.2f}"
                })
            else:
                pbar.set_postfix({"loss": f"{loss.item():.4f}"})

        # é˜²æ­¢é™¤ä»¥é›¶
        if valid_steps > 0:
            avg_loss = epoch_loss / valid_steps
        else:
            avg_loss = float("nan")
            
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, valid_steps={valid_steps}, skipped_steps={skipped_steps}")

    torch.save(model.state_dict(), "clip_sues.pth")
    print("âœ… Model saved as clip_sues.pth")

@torch.no_grad()
def test():
    model, preprocess = clip.load(
        "ViT-B/32",
        device=device,
        download_root="../../clip_weights",
        jit=False
    )
    model.load_state_dict(torch.load("clip_sues.pth", map_location=device))
    model.eval()

    # ================= 1. åŠ è½½æµ‹è¯•æ•°æ®æ–‡æœ¬å’ŒGT sceneï¼ˆpaired æ ‡æ³¨ï¼‰ =================
    test_dataset = CLIPPairedDataset(test_json, image_root, test_transform)
    texts = test_dataset.texts
    gt_scenes = test_dataset.gt_scenes

    # ================= 2. æ”¶é›†æ— äººæœºå›¾åº“ï¼ˆå…¨éƒ¨å›¾ç‰‡ï¼‰ =================
    drone_root = os.path.join(image_root, "drone_view_512")
    drone_paths = []
    drone_scenes = []
    for root_dir, dirs, files in os.walk(drone_root):
        for f in files:
            if f.lower().endswith(".jpg"):
                full_path = os.path.join(root_dir, f)
                rel = os.path.relpath(full_path, image_root).replace("\\", "/")
                drone_paths.append(full_path)
                drone_scenes.append(parse_scene_from_path(rel))

    # ================= 3. æ”¶é›†å«æ˜Ÿå›¾åº“ï¼ˆæ¯åœºæ™¯ä¸€å¼ å«æ˜Ÿå›¾ï¼Œscene/0.pngï¼‰ =================
    satellite_root = os.path.join(image_root, "satellite-view")
    satellite_paths = []
    satellite_scenes = []
    for scene in os.listdir(satellite_root):
        scene_dir = os.path.join(satellite_root, scene)
        if not os.path.isdir(scene_dir):
            continue
        img_path = os.path.join(scene_dir, "0.png")
        if os.path.exists(img_path):
            satellite_paths.append(img_path)
            satellite_scenes.append(scene)

    # ================ 4. æå–æ— äººæœºå›¾åƒç‰¹å¾ï¼ˆbatchï¼‰ ================
    print("âœ… Extracting drone image features...")
    drone_feats_list = []
    bs_img = batch_size
    for i in tqdm(range(0, len(drone_paths), bs_img)):
        batch_paths = drone_paths[i: i + bs_img]
        imgs = []
        for p in batch_paths:
            img = Image.open(p).convert("RGB")
            img = test_transform(img)
            imgs.append(img)
        imgs = torch.stack(imgs).to(device)
        feats = model.encode_image(imgs)
        feats = l2norm(feats, NORM_EPS)
        drone_feats_list.append(feats.cpu())
    if len(drone_feats_list) == 0:
        raise RuntimeError("No drone images found in: " + drone_root)
    drone_feats = torch.cat(drone_feats_list, dim=0)

    # ================ 5. æå–å«æ˜Ÿå›¾åƒç‰¹å¾ï¼ˆbatchï¼‰ ================
    print("âœ… Extracting satellite image features...")
    sat_feats_list = []
    for i in tqdm(range(0, len(satellite_paths), bs_img)):
        batch_paths = satellite_paths[i: i + bs_img]
        imgs = []
        for p in batch_paths:
            img = Image.open(p).convert("RGB")
            img = test_transform(img)
            imgs.append(img)
        imgs = torch.stack(imgs).to(device)
        feats = model.encode_image(imgs)
        feats = l2norm(feats, NORM_EPS)
        sat_feats_list.append(feats.cpu())
    if len(sat_feats_list) == 0:
        raise RuntimeError("No satellite images found in: " + satellite_root)
    satellite_feats = torch.cat(sat_feats_list, dim=0)

    # ================ 6. æå–æ–‡æœ¬ç‰¹å¾ï¼ˆbatchï¼‰ ================
    print("âœ… Extracting text features...")
    text_feats_list = []
    for i in tqdm(range(0, len(texts), bs_img)):
        batch_texts = texts[i: i + bs_img]
        tokens = clip.tokenize(batch_texts, truncate=True).to(device)
        feats = model.encode_text(tokens)
        feats = l2norm(feats, NORM_EPS)
        text_feats_list.append(feats.cpu())
    text_feats = torch.cat(text_feats_list, dim=0)

    # ================ 7. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆGPUä¸Šç®—ï¼Œå†è½¬CPUï¼‰ ================
    print("âœ… Computing similarity matrices...")
    text_feats_gpu = text_feats.to(device)
    drone_feats_gpu = drone_feats.to(device)
    sat_feats_gpu = satellite_feats.to(device)

    sims_text_drone = (text_feats_gpu @ drone_feats_gpu.t()).cpu().numpy()
    sims_text_sat   = (text_feats_gpu @ sat_feats_gpu.t()).cpu().numpy()

    # ================ 8. è¯„ä¼° Top-1/5/10ï¼ˆscene-levelï¼‰ ================
    ks = [1, 5, 10]
    drone_counts = {k: 0 for k in ks}
    sat_counts = {k: 0 for k in ks}
    combined_counts = {k: 0 for k in ks}
    total = len(texts)

    for i in range(total):
        gt_scene = gt_scenes[i]

        ranks_drone = np.argsort(sims_text_drone[i])[::-1]
        ranks_sat   = np.argsort(sims_text_sat[i])[::-1]

        for k in ks:
            topk_dr = ranks_drone[:k]
            topk_sat = ranks_sat[:k]

            pred_drone_scenes = [drone_scenes[idx] for idx in topk_dr]
            pred_sat_scenes = [satellite_scenes[idx] for idx in topk_sat]

            ok_drone = gt_scene in pred_drone_scenes
            ok_sat   = gt_scene in pred_sat_scenes

            if ok_drone:
                drone_counts[k] += 1
            if ok_sat:
                sat_counts[k] += 1
            if ok_drone and ok_sat:
                combined_counts[k] += 1

    # ================ 9. è¾“å‡ºç»“æœ ================
    print("\nğŸ“Š Retrieval Results (scene-level):")
    for k in ks:
        d_acc = drone_counts[k] / total * 100.0
        s_acc = sat_counts[k] / total * 100.0
        c_acc = combined_counts[k] / total * 100.0
        print(f"Top-{k}: Drone acc = {d_acc:.2f}%, Satellite acc = {s_acc:.2f}%, Both acc = {c_acc:.2f}%")

    return {
        "drone_counts": drone_counts,
        "sat_counts": sat_counts,
        "combined_counts": combined_counts,
        "total": total,
        "drone_paths": drone_paths,
        "drone_scenes": drone_scenes,
        "satellite_paths": satellite_paths,
        "satellite_scenes": satellite_scenes
    }


if __name__ == "__main__":
    #train()
    test()