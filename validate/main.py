import os
import json
import torch
import clip
import re
import numpy as np
from PIL import Image
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

# ============ é…ç½® ============
device = "cuda" if torch.cuda.is_available() else "cpu"

image_root = r"D:\dataset-w\SUES-200-512x512-V2\SUES-200-512x512"
train_json = "./train.json"
test_json  = "./test.json"

batch_size = 16                     # âœ… æé«˜ batch
epochs = 20                         # âœ… å¢åŠ è®­ç»ƒè½®æ•°
lr = 1e-5
num_workers = 4

# ============ æ–‡æœ¬æ¸…æ´— ============
def clean_text(s: str) -> str:
    if s is None:
        return ""
    if not isinstance(s, str):
        s = str(s)

    s = re.sub(r"(?i)\bWord\s*count\s*:\s*\d+\.?$", "", s.strip())
    s = re.sub(r"\s+", " ", s).strip()
    return s

def build_prompt(text):
    # âœ… æ”¹è¿›æˆ CLIP æ›´å‹å¥½çš„ prompt
    return f"aerial image: {text}"

# ============ æ•°æ®é›† ============
class CLIPDataset(Dataset):
    def __init__(self, json_path, image_root, preprocess):
        with open(json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        self.image_root = image_root
        self.keys = []
        self.texts = []
        self.preprocess = preprocess

        for item in self.data:
            for k, v in item.items():
                self.keys.append(k)
                self.texts.append(build_prompt(clean_text(v)))

    def __len__(self):
        return len(self.keys)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_root, self.keys[idx])
        text = self.texts[idx]

        image = Image.open(img_path).convert("RGB")
        image = self.preprocess(image)
        return image, text

# ============ è®­ç»ƒ ============
def train():
    model, preprocess = clip.load(
        "ViT-B/32",
        device=device,
        download_root="../clip_weights",
        jit=False
    )

    train_dataset = CLIPDataset(train_json, image_root, preprocess)
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    loss_fn = torch.nn.CrossEntropyLoss()

    print("âœ… Start Training")

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for images, texts in tqdm(train_loader):
            images = images.to(device)
            texts = [t if isinstance(t, str) else str(t) for t in texts]
            tokens = clip.tokenize(texts, truncate=True).to(device)

            # âœ… æ­£ç¡®çš„ CLIP forward
            logits_per_image, logits_per_text = model(images, tokens)

            labels = torch.arange(len(images)).long().to(device)

            loss = (loss_fn(logits_per_image, labels) +
                    loss_fn(logits_per_text, labels)) / 2

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch [{epoch+1}/{epochs}] Loss: {total_loss/len(train_loader):.4f}")

    torch.save(model.state_dict(), "clip_sues.pth")
    print("âœ… Model saved")

# ============ æµ‹è¯• ============
@torch.no_grad()
def test():
    model, preprocess = clip.load(
        "ViT-B/32",
        device=device,
        download_root="../clip_weights",
        jit=False
    )

    model.load_state_dict(torch.load("clip_sues.pth", map_location=device))
    model.eval()

    test_dataset = CLIPDataset(test_json, image_root, preprocess)
    texts = test_dataset.texts

    # è§£æ GT scene
    gt_scenes = []
    for k in test_dataset.keys:
        parts = k.replace("\\", "/").split("/")
        gt_scenes.append(parts[1] if len(parts) > 1 else "")

    # æ”¶é›†æ— äººæœºå›¾åº“
    drone_root = os.path.join(image_root, "drone_view_512")
    drone_paths, drone_scenes = [], []
    for r, d, f_list in os.walk(drone_root):
        for f in f_list:
            if f.lower().endswith(".jpg"):
                full = os.path.join(r, f)
                rel = os.path.relpath(full, image_root).replace("\\", "/")
                parts = rel.split("/")
                drone_paths.append(full)
                drone_scenes.append(parts[1] if len(parts) > 1 else "")

    # æ”¶é›†å«æ˜Ÿå›¾åº“
    satellite_root = os.path.join(image_root, "satellite-view")
    sat_paths, sat_scenes = [], []
    for scene in os.listdir(satellite_root):
        p = os.path.join(satellite_root, scene, "0.png")
        if os.path.exists(p):
            sat_paths.append(p)
            sat_scenes.append(scene)

    # å›¾åƒç‰¹å¾æå–
    def encode_images(paths):
        feats = []
        for i in tqdm(range(0, len(paths), batch_size)):
            batch = paths[i:i+batch_size]
            imgs = [preprocess(Image.open(p).convert("RGB")) for p in batch]
            imgs = torch.stack(imgs).to(device)

            f = model.encode_image(imgs)
            f = f / f.norm(dim=-1, keepdim=True)
            feats.append(f.cpu())
        return torch.cat(feats, dim=0)

    print("âœ… Encoding drone images")
    drone_feats = encode_images(drone_paths)

    print("âœ… Encoding satellite images")
    sat_feats = encode_images(sat_paths)

    # æ–‡æœ¬ç‰¹å¾
    text_feats = []
    for i in tqdm(range(0, len(texts), batch_size)):
        bt = texts[i:i+batch_size]
        tokens = clip.tokenize(bt, truncate=True).to(device)
        f = model.encode_text(tokens)
        f = f / f.norm(dim=-1, keepdim=True)
        text_feats.append(f.cpu())
    text_feats = torch.cat(text_feats, dim=0)

    # ç›¸ä¼¼åº¦
    sims_drone = (text_feats @ drone_feats.t()).numpy()
    sims_sat   = (text_feats @ sat_feats.t()).numpy()

    # è¯„ä¼°
    ks = [1, 5, 10]
    res = {k: {"drone":0, "sat":0, "both":0} for k in ks}
    total = len(texts)

    for i in range(total):
        gt = gt_scenes[i]

        rank_dr = np.argsort(sims_drone[i])[::-1]
        rank_sa = np.argsort(sims_sat[i])[::-1]

        for k in ks:
            pred_dr = [drone_scenes[x] for x in rank_dr[:k]]
            pred_sa = [sat_scenes[x] for x in rank_sa[:k]]

            ok_dr = gt in pred_dr
            ok_sa = gt in pred_sa

            if ok_dr:
                res[k]["drone"] += 1
            if ok_sa:
                res[k]["sat"] += 1
            if ok_dr and ok_sa:
                res[k]["both"] += 1

    print("\nğŸ“Š Results:")
    for k in ks:
        print(
            f"Top-{k} | Drone: {res[k]['drone']/total*100:.2f}% "
            f"Satellite: {res[k]['sat']/total*100:.2f}% "
            f"Both: {res[k]['both']/total*100:.2f}%"
        )

# ============ ä¸»å…¥å£ ============
if __name__ == "__main__":
    train()
    test()


# import os
# import json
# import torch
# import clip
# import re
# import numpy as np
# from PIL import Image
# from tqdm import tqdm
# from torch.utils.data import Dataset, DataLoader

# # ============ é…ç½® ============
# device = "cuda" if torch.cuda.is_available() else "cpu"

# image_root = r"D:\dataset-w\SUES-200-512x512-V2\SUES-200-512x512"               # æ”¹æˆä½ çš„å›¾åƒæ ¹ç›®å½•
# train_json = "./train.json"     # ä½ çš„è®­ç»ƒæ ‡æ³¨
# test_json  = "./test.json"      # ä½ çš„æµ‹è¯•æ ‡æ³¨

# batch_size = 2
# epochs = 5
# lr = 1e-5
# num_workers = 4

# def truncate_text(t, max_len=70):
#     words = t.split()
#     if len(words) <= max_len:
#         return t
#     return " ".join(words[:max_len])

# # ============ æ•°æ®é›† ============
# # class CLIPDataset(Dataset):
# #     def __init__(self, json_path, image_root):
# #         with open(json_path, 'r', encoding='utf-8') as f:
# #             self.data = json.load(f)

# #         self.image_root = image_root
# #         self.keys = []
# #         self.texts = []

# #         for item in self.data:
# #             for k, v in item.items():
# #                 self.keys.append(k)
# #                 self.texts.append(v)

# #     def __len__(self):
# #         return len(self.keys)

# #     def __getitem__(self, idx):
# #         img_path = os.path.join(self.image_root, self.keys[idx])
# #         text = self.texts[idx]

# #         image = Image.open(img_path).convert("RGB")
# #         return image, text

# def clean_text(s: str) -> str:
#     """
#     æ¸…æ´—æ–‡æœ¬ï¼š
#     - å»æ‰ 'Word count: 142' è¿™ç§å°¾éƒ¨è®¡æ•°ä¿¡æ¯
#     - åˆå¹¶å¤šç©ºç™½ä¸ºå•ä¸ªç©ºæ ¼
#     - å»æ‰å¼€å¤´/ç»“å°¾ç©ºç™½
#     - ä¿è¯æ˜¯å­—ç¬¦ä¸²
#     """
#     if s is None:
#         return ""
#     if not isinstance(s, str):
#         s = str(s)

#     # å»æ‰ç±»ä¼¼ "Word count: 142." æˆ– "Word count: 142" çš„å°¾éƒ¨è¡Œ
#     s = re.sub(r"(?i)\bWord\s*count\s*:\s*\d+\.?$", "", s.strip())

#     # å»æ‰å…¶ä»–æœ«å°¾å½¢å¼çš„â€œWord countâ€å‡ºç°åœ¨å¥å°¾çš„æƒ…å†µ
#     s = re.sub(r"(?i)\s*Word\s*count\s*[:\-]\s*\d+\s*\.?$", "", s)

#     # åˆå¹¶å¤šç©ºç™½å’Œæ¢è¡Œ
#     s = re.sub(r"\s+", " ", s).strip()

#     return s

# class CLIPDataset(Dataset):
#     def __init__(self, json_path, image_root, preprocess):
#         with open(json_path, 'r', encoding='utf-8') as f:
#             self.data = json.load(f)

#         self.image_root = image_root
#         self.keys = []
#         self.texts = []
#         self.preprocess = preprocess

#         for item in self.data:
#             for k, v in item.items():
#                 self.keys.append(k)
#                 # è¿™é‡Œç›´æ¥æ¸…æ´—å¹¶ä¿å­˜æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œé¿å… worker é—®é¢˜
#                 self.texts.append(clean_text(v))

#     def __len__(self):
#         return len(self.keys)

#     def __getitem__(self, idx):
#         img_path = os.path.join(self.image_root, self.keys[idx])
#         text = self.texts[idx]

#         image = Image.open(img_path).convert("RGB")
#         image = self.preprocess(image)   # ç›´æ¥è¿”å› tensor

#         return image, text


# # ============ Training ============
# def train():
#     model, preprocess = clip.load(
#         "ViT-B/32",
#         device=device,
#         download_root="../clip_weights",
#         jit=False)

#     # train_dataset = CLIPDataset(train_json, image_root)
#     train_dataset = CLIPDataset(train_json, image_root, preprocess)
#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)

#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)
#     loss_fn = torch.nn.CrossEntropyLoss()

#     print("âœ… Start Training")

#     for epoch in range(epochs):
#         model.train()
#         epoch_loss = 0

#         for images, texts in tqdm(train_loader):
#             # images = torch.stack([preprocess(img) for img in images]).to(device)
#             images = images.to(device)
#             # texts = clip.tokenize(texts).to(device)
#             # texts ç°åœ¨æ˜¯ä¸€ä¸ª list[str]ï¼Œå…ˆåšé¢å¤–çš„é˜²æŠ¤æ€§æ¸…æ´—ï¼ˆå¯é€‰ï¼‰
#             texts = [t if isinstance(t, str) else str(t) for t in texts]

#             # ä½¿ç”¨ CLIP å®˜æ–¹æä¾›çš„ token-level æˆªæ–­ï¼ˆæœ€ç¨³å¦¥ï¼‰
#             tokens = clip.tokenize(texts, truncate=True).to(device)

#             image_feat, text_feat = model(images, tokens)

#             logits_per_image = image_feat @ text_feat.t()
#             logits_per_text = logits_per_image.t()

#             labels = torch.arange(len(images)).to(device)

#             loss = (loss_fn(logits_per_image, labels) +
#                     loss_fn(logits_per_text, labels)) / 2

#             optimizer.zero_grad()
#             loss.backward()
#             optimizer.step()

#             epoch_loss += loss.item()

#         print(f"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}")

#     torch.save(model.state_dict(), "clip_sues.pth")
#     print("âœ… Model saved as clip_sues.pth")


# # ============ Evaluation ============
# # @torch.no_grad()
# # def test():
# #     model, preprocess = clip.load(
# #         "ViT-B/32",
# #         device=device,
# #         download_root="./clip_weights",
# #         jit=False
# #     )
# #     model.load_state_dict(torch.load("clip_sues.pth", map_location=device))
# #     model.eval()

# #     # ================= 1. åŠ è½½æ–‡æœ¬æ•°æ® =================
# #     test_dataset = CLIPDataset(test_json, image_root)
# #     texts = test_dataset.texts

# #     # ================= 2. æ„å»ºæ— äººæœºå›¾åº“ =================
# #     drone_root = os.path.join(image_root, "drone_view_512")
# #     satellite_root = os.path.join(image_root, "satellite-view")

# #     drone_paths = []
# #     drone_scenes = []

# #     for scene in os.listdir(drone_root):
# #         scene_dir = os.path.join(drone_root, scene)
# #         if not os.path.isdir(scene_dir):
# #             continue

# #         # æ‰¾ä»»æ„ä¸€å¼ æ— äººæœºå›¾ä½œä¸ºä»£è¡¨
# #         for root, dirs, files in os.walk(scene_dir):
# #             for f in files:
# #                 if f.endswith(".jpg"):
# #                     full_path = os.path.join(root, f)
# #                     drone_paths.append(full_path)
# #                     drone_scenes.append(scene)
# #                     break
# #             if len(drone_paths) > 0 and drone_scenes[-1] == scene:
# #                 break

# #     # ================= 3. æ„å»ºå«æ˜Ÿå›¾åº“ =================
# #     satellite_paths = []
# #     satellite_scenes = []

# #     for scene in os.listdir(satellite_root):
# #         scene_dir = os.path.join(satellite_root, scene)
# #         if not os.path.isdir(scene_dir):
# #             continue

# #         img_path = os.path.join(scene_dir, f"{scene}.jpg")
# #         if os.path.exists(img_path):
# #             satellite_paths.append(img_path)
# #             satellite_scenes.append(scene)

# #     # ================= 4. æå–æ— äººæœºç‰¹å¾ =================
# #     print("âœ… Extracting drone image features...")
# #     drone_feats = []

# #     for p in tqdm(drone_paths):
# #         img = preprocess(Image.open(p).convert("RGB")).unsqueeze(0).to(device)
# #         feat = model.encode_image(img)
# #         feat = feat / feat.norm(dim=-1, keepdim=True)
# #         drone_feats.append(feat.cpu())

# #     drone_feats = torch.cat(drone_feats, dim=0)

# #     # ================= 5. æå–å«æ˜Ÿç‰¹å¾ =================
# #     print("âœ… Extracting satellite image features...")
# #     satellite_feats = []

# #     for p in tqdm(satellite_paths):
# #         img = preprocess(Image.open(p).convert("RGB")).unsqueeze(0).to(device)
# #         feat = model.encode_image(img)
# #         feat = feat / feat.norm(dim=-1, keepdim=True)
# #         satellite_feats.append(feat.cpu())

# #     satellite_feats = torch.cat(satellite_feats, dim=0)

# #     # ================= 6. æå–æ–‡æœ¬ç‰¹å¾ =================
# #     print("âœ… Extracting text features...")
# #     text_feats = []

# #     for t in tqdm(texts):
# #         token = clip.tokenize(t).to(device)
# #         feat = model.encode_text(token)
# #         feat = feat / feat.norm(dim=-1, keepdim=True)
# #         text_feats.append(feat.cpu())

# #     text_feats = torch.cat(text_feats, dim=0)

# #     # ================= 7. ç›¸ä¼¼åº¦è®¡ç®— =================
# #     sims_text_drone = text_feats @ drone_feats.t()
# #     sims_text_sat   = text_feats @ satellite_feats.t()

# #     sims_text_drone = sims_text_drone.numpy()
# #     sims_text_sat   = sims_text_sat.numpy()

# #     # ================= 8. è¯„ä¼°é€»è¾‘ =================
# #     success = 0
# #     total = len(texts)

# #     for i in range(total):
# #         # æ–‡æœ¬çœŸå® sceneï¼ˆæ¥è‡ªåŸå§‹æ ‡æ³¨è·¯å¾„ï¼‰
# #         gt_path = test_dataset.keys[i]
# #         gt_scene = gt_path.split("/")[1]   # drone_view_512/0001/...

# #         # æ— äººæœºæ£€ç´¢
# #         best_drone_idx = np.argmax(sims_text_drone[i])
# #         pred_drone_scene = drone_scenes[best_drone_idx]

# #         # å«æ˜Ÿæ£€ç´¢
# #         best_sat_idx = np.argmax(sims_text_sat[i])
# #         pred_sat_scene = satellite_scenes[best_sat_idx]

# #         if pred_drone_scene == gt_scene and pred_sat_scene == gt_scene:
# #             success += 1

# #     print("\nğŸ“Š Cross-view Retrieval Result:")
# #     print(f"Total samples: {total}")
# #     print(f"Both-view correct: {success}")
# #     print(f"Accuracy: {success/total*100:.2f}%")
# @torch.no_grad()
# def test():
#     model, preprocess = clip.load(
#         "ViT-B/32",
#         device=device,
#         download_root="../clip_weights",
#         jit=False
#     )
#     model.load_state_dict(torch.load("clip_sues.pth", map_location=device))
#     model.eval()

#     # ================= 1. åŠ è½½æµ‹è¯•æ•°æ®æ–‡æœ¬å’ŒGT scene =================
#     test_dataset = CLIPDataset(test_json, image_root,preprocess)
#     texts = test_dataset.texts
#     gt_scenes = []
#     # ä»æ•°æ®é‡Œçš„ key æ¨ sceneï¼ˆå‡è®¾ key æ ¼å¼ä¸º "drone_view_512/0001/150/1.jpg"ï¼‰
#     for k in test_dataset.keys:
#         parts = k.split("/")
#         if len(parts) >= 2:
#             gt_scenes.append(parts[1])
#         else:
#             gt_scenes.append("")  # å®¹é”™

#     # ================= 2. æ”¶é›†æ— äººæœºå›¾åº“ï¼ˆå…¨éƒ¨å›¾ç‰‡ï¼‰ =================
#     drone_root = os.path.join(image_root, "drone_view_512")
#     drone_paths = []
#     drone_scenes = []
#     for root_dir, dirs, files in os.walk(drone_root):
#         for f in files:
#             if f.lower().endswith(".jpg"):
#                 full_path = os.path.join(root_dir, f)
#                 # ç›¸å¯¹ image_root çš„è·¯å¾„å½¢å¼
#                 rel = os.path.relpath(full_path, image_root)
#                 parts = rel.replace("\\", "/").split("/")  # å…¼å®¹ Windows è·¯å¾„åˆ†éš”
#                 # æœŸæœ› parts = ['drone_view_512','0001','150','1.jpg']
#                 if len(parts) >= 2:
#                     scene = parts[1]
#                 else:
#                     scene = ""
#                 drone_paths.append(full_path)
#                 drone_scenes.append(scene)

#     # ================= 3. æ”¶é›†å«æ˜Ÿå›¾åº“ï¼ˆæ¯åœºæ™¯ä¸€å¼ å«æ˜Ÿå›¾ï¼Œæ ¼å¼ï¼šscene/scene.jpgï¼‰ =================
#     satellite_root = os.path.join(image_root, "satellite-view")
#     satellite_paths = []
#     satellite_scenes = []
#     for scene in os.listdir(satellite_root):
#         scene_dir = os.path.join(satellite_root, scene)
#         if not os.path.isdir(scene_dir):
#             continue
#         img_path = os.path.join(scene_dir, "0.png")
#         if os.path.exists(img_path):
#             satellite_paths.append(img_path)
#             satellite_scenes.append(scene)

#     # ================ 4. æå–æ— äººæœºå›¾åƒç‰¹å¾ï¼ˆæŒ‰ batch æå–ï¼Œé¿å…é€å¼ è¿‡æ…¢ï¼‰ ================
#     print("âœ… Extracting drone image features...")
#     drone_feats_list = []
#     bs_img = batch_size  # é‡ç”¨è®­ç»ƒçš„ batch_size
#     for i in tqdm(range(0, len(drone_paths), bs_img)):
#         batch_paths = drone_paths[i: i + bs_img]
#         imgs = []
#         for p in batch_paths:
#             img = Image.open(p).convert("RGB")
#             imgs.append(preprocess(img))
#         imgs = torch.stack(imgs).to(device)
#         feats = model.encode_image(imgs)
#         feats = feats / feats.norm(dim=-1, keepdim=True)
#         drone_feats_list.append(feats.cpu())
#     if len(drone_feats_list) == 0:
#         raise RuntimeError("No drone images found in: " + drone_root)
#     drone_feats = torch.cat(drone_feats_list, dim=0)  # [N_drone, D]

#     # ================ 5. æå–å«æ˜Ÿå›¾åƒç‰¹å¾ï¼ˆæŒ‰ batchï¼‰ ================
#     print("âœ… Extracting satellite image features...")
#     sat_feats_list = []
#     for i in tqdm(range(0, len(satellite_paths), bs_img)):
#         batch_paths = satellite_paths[i: i + bs_img]
#         imgs = []
#         for p in batch_paths:
#             img = Image.open(p).convert("RGB")
#             imgs.append(preprocess(img))
#         imgs = torch.stack(imgs).to(device)
#         feats = model.encode_image(imgs)
#         feats = feats / feats.norm(dim=-1, keepdim=True)
#         sat_feats_list.append(feats.cpu())
#     if len(sat_feats_list) == 0:
#         raise RuntimeError("No satellite images found in: " + satellite_root)
#     satellite_feats = torch.cat(sat_feats_list, dim=0)  # [N_sat, D]

#     # ================ 6. æå–æ–‡æœ¬ç‰¹å¾ï¼ˆæŒ‰ batchï¼‰ ================
#     print("âœ… Extracting text features...")
#     text_feats_list = []
#     for i in tqdm(range(0, len(texts), bs_img)):
#         batch_texts = texts[i: i + bs_img]
#         tokens = clip.tokenize(batch_texts, truncate=True).to(device)  # returns tensor [B, L]
#         feats = model.encode_text(tokens)
#         feats = feats / feats.norm(dim=-1, keepdim=True)
#         text_feats_list.append(feats.cpu())
#     text_feats = torch.cat(text_feats_list, dim=0)  # [N_text, D]

#     # ================ 7. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆæ–‡æœ¬ vs droneï¼‰ï¼Œï¼ˆæ–‡æœ¬ vs satelliteï¼‰ ================
#     print("âœ… Computing similarity matrices...")
#     sims_text_drone = (text_feats @ drone_feats.t()).numpy()      # [N_text, N_drone]
#     sims_text_sat   = (text_feats @ satellite_feats.t()).numpy()  # [N_text, N_sat]

#     # ================ 8. è¯„ä¼° Top-1/5/10ï¼ˆåŒæ—¶ç»Ÿè®¡å•è§†è§’å’Œä¸¤è§†è§’åŒæ—¶æˆåŠŸï¼‰ ================
#     ks = [1, 5, 10]
#     drone_counts = {k: 0 for k in ks}
#     sat_counts = {k: 0 for k in ks}
#     combined_counts = {k: 0 for k in ks}
#     total = len(texts)

#     for i in range(total):
#         gt_scene = gt_scenes[i]

#         # drone top-k
#         ranks_drone = np.argsort(sims_text_drone[i])[::-1]  # å¤§åˆ°å°
#         # satellite top-k
#         ranks_sat = np.argsort(sims_text_sat[i])[::-1]

#         for k in ks:
#             topk_dr = ranks_drone[:k]
#             topk_sat = ranks_sat[:k]

#             # check if any of topk_dr scenes equals gt_scene
#             pred_drone_scenes = [drone_scenes[idx] for idx in topk_dr]
#             pred_sat_scenes = [satellite_scenes[idx] for idx in topk_sat]

#             ok_drone = gt_scene in pred_drone_scenes
#             ok_sat = gt_scene in pred_sat_scenes

#             if ok_drone:
#                 drone_counts[k] += 1
#             if ok_sat:
#                 sat_counts[k] += 1
#             if ok_drone and ok_sat:
#                 combined_counts[k] += 1

#     # ================ 9. è¾“å‡ºç»“æœ ================
#     print("\nğŸ“Š Retrieval Results (scene-level):")
#     for k in ks:
#         d_acc = drone_counts[k] / total * 100.0
#         s_acc = sat_counts[k] / total * 100.0
#         c_acc = combined_counts[k] / total * 100.0
#         print(f"Top-{k}: Drone acc = {d_acc:.2f}%, Satellite acc = {s_acc:.2f}%, Both acc = {c_acc:.2f}%")

#     # ä¹Ÿè¿”å›è¯¦ç»†æ•°å€¼ä»¥ä¾¿åç»­å¤„ç†
#     return {
#         "drone_counts": drone_counts,
#         "sat_counts": sat_counts,
#         "combined_counts": combined_counts,
#         "total": total,
#         "drone_paths": drone_paths,
#         "drone_scenes": drone_scenes,
#         "satellite_paths": satellite_paths,
#         "satellite_scenes": satellite_scenes
#     }


# # ============ Main ============
# if __name__ == "__main__":
#     #train()
#     test()
